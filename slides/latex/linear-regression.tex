\documentclass{clbeamer2024}

\title{\textbf{Linear Regression}}
\subtitle{Interpretation of coeficients}
\author{Arturo Soberon}
\institute{}
\date{}

\begin{document}
\setcounter{framenumber}{-1}
\frame{\titlepage}

% Assumptions
\begin{frame}{Assumptions}
    The observed outcome $Y_i$ is composed of a deterministic process and a random process.
    \begin{equation}
        Y_i = f(X_i) + \varepsilon_i
    \end{equation}

    We model its Conditional Expectation Function with a linear function, assuming that $\varepsilon_i \perp \varepsilon_j$.
    \begin{equation}
        E(Y_i | X_i) = X_i^T \beta
    \end{equation}
\end{frame}

% Testing
\begin{frame}{Linear Regression VS differences in means}
    We can compare the means of two groups to estimate the causal effect of a treatment.
    \begin{equation*}
        D_i = 
        \begin{cases}
            0, \text{$i$ is not treated} \\
            1, \text{$i$ is treated}
        \end{cases}
    \end{equation*}

    \begin{equation}
        \rho = E(Y_i | D_i = 1) - E(T_i | D_i = 0)
    \end{equation}

    We can scale it up a notch with linear regression:
    \begin{equation}
        E(Y_i | X_i, D_i) = X_i^T \gamma + D_i \rho 
    \end{equation}
    Where $X_i$ are \textit{good} controls ($X_i \perp D_i$) that help reduce residual variance.
\end{frame}

\begin{frame}{Linear Regression VS differences in means}
    To see why linear regression can help us estimate the causal effect:
    \begin{equation*}
    \begin{split}
        E(Y_i | X_i, D_i = 1) & = X_i^T \gamma + \rho \\
        E(Y_i | X_i, D_i = 0) & = X_i^T \gamma \\
        \implies \rho & = E(Y_i | X_i, D_i = 1) - E(Y_i | X_i, D_i = 0)
    \end{split}
    \end{equation*}
    So essentially, we're interested in finding good controls and a treatment that is independent to $X$ and $\varepsilon$ to estimate $\rho$ using linear regression.
\end{frame}

% Interpretation (level-level)
\begin{frame}{Level-level}
    Let $Y_i$ be the observed income of the $i$-th individual.
    \begin{equation*}
        Y_i = \beta_0 + \beta_1 \textit{Age}_i + \beta_2 \textit{College}_i + \varepsilon_i
    \end{equation*}
    Since $\varepsilon$ and going to college are arguably not independent, then we cannot think of this model as a causal one.

    \pause
    \begin{enumerate}
        \item On average, a marginal increase in Age is \textbf{associated} with a $\beta_1$ increase in one's income.
        \item On average, going to college is \textbf{associated} with a $\beta_2$ increase in one's income.
    \end{enumerate}
\end{frame}

% Interpretation (log-level)
\begin{frame}{Log-level}
    Let $Y_i$ be the observed income of the $i$-th individual.
    \begin{equation*}
        \log(Y_i) = \beta_0 + \beta_1 \textit{Age}_i + \beta_2 \textit{College}_i + \varepsilon_i
    \end{equation*}

    \pause
    \begin{enumerate}
        \item On average, a \textbf{one percent increase} increase in Age is \textbf{associated} with a $\frac{\beta_1}{100}$ increase in one's income.
        \item On average, $\beta_2$ represents the difference in expected log-income of college graduates and non-graduates (no point interpreting a one-percent increase in attending college).
    \end{enumerate}
\end{frame}

% Interpretation (log-log)
\begin{frame}{Log-log}
    Let $Y_i$ be the observed income of the $i$-th individual.
    \begin{equation*}
        \log(Y_i) = \beta_0 + \beta_1 \log(\textit{Age}_i) + \beta_2 \textit{College}_i + \varepsilon_i
    \end{equation*}

    \pause
    \begin{enumerate}
        \item On average, a \textbf{one percent increase} increase in Age is \textbf{associated} with a $\beta_1$ \textbf{percent increase} in one's income.
        \item On average, $\beta_2$ represents the difference in expected log-income of college graduates and non-graduates (no point interpreting a one-percent increase in attending college).
    \end{enumerate}
\end{frame}

% Interpretation (interaction cont bin)
\begin{frame}{Interaction (Continuous \& Binary)}
    Let $Y_i$ be the observed income of the $i$-th individual.
    \begin{equation*}
        Y_i = \beta_0 + \beta_1 \textit{Age}_i + \beta_2 \textit{College}_i
            + \beta_3 \textit{Age}_i \times \textit{College}_i + \varepsilon_i
    \end{equation*}
    This model hypothesizes that age has a \textit{heterogeneous} impact on both groups. To see this:
    
    \begin{equation*}
    \begin{split}
        E(Y_i | \textit{College}_i = 1) & = \beta_0 + \beta_1 \textit{Age}_i + \beta_2 + \beta_3 \textit{Age}_i \\
            & = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) \textit{Age}_i \\
        E(Y_i | \textit{College}_i = 0) & = \beta_0 + \beta_1 \textit{Age}_i \\
        \implies E(Y_{1i}) - E(Y_{0i}) & = \beta_2 + \beta_3 \textit{Age}_i
    \end{split}
    \end{equation*}
    It's as if age had a different effect on graduates' income:
    \begin{equation*}
        \frac{\delta \big(E(Y_{1i}) - E(Y_{0i}) \big)}{\delta \textit{Age}} = \beta_3
    \end{equation*}
\end{frame}

% Interpretation (bin bin)
\begin{frame}{Interaction (Binary \& Binary)}
    \begin{equation*}
        Y_i = \beta_0 + \beta_1 \textit{Time}_i + \beta_2 \textit{Treatment}_i
            + \beta_3 \textit{Time}_i \times \textit{Treatment}_i + \varepsilon_i
    \end{equation*}

    This model is the same as having a conditional mean for all four combinations.
    \begin{equation*}
    \begin{split}
        E(Y_i | \textit{Time}_i = 0, \textit{Treatment}_i = 0) & = \beta_0 \\
        E(Y_i | \textit{Time}_i = 1, \textit{Treatment}_i = 0) & = \beta_0 + \beta_1 \\
        E(Y_i | \textit{Time}_i = 0, \textit{Treatment}_i = 1) & = \beta_0 + \beta_2 \\
        E(Y_i | \textit{Time}_i = 1, \textit{Treatment}_i = 1) & = \beta_0 + \beta_1 + \beta_2 + \beta_3 \\
    \end{split}
    \end{equation*}

    \begin{center}
    \fbox{$\beta_3$ is known as a differences-in-differences estimator!}
    \end{center}

    \pause
    \begin{equation*}
    \begin{split}
        \beta_3 = & \Big(
            E(Y_i | \textit{Time}\,1, \textit{Treatment}\,1)
            - E(Y_i | \textit{Time}\,0, \textit{Treatment}\,1)
        \Big) \\
        & - \Big(
            E(Y_i | \textit{Time}\,1, \textit{Treatment}\,0)
            - E(Y_i | \textit{Time}\,0, \textit{Treatment}\,0)
        \Big)
    \end{split}
    \end{equation*}
\end{frame}

% Interpretation of multiple categories
\begin{frame}{Multi-level Categorical Features}
    Suppose our population can live in one of three regions. We can encode categorical variable $Area$ as follows:
    \begin{equation*}
        Area_i = \begin{cases}
            \textit{Rural}_i = 1 \text{ if $i$ lives in a Rural Area and $0$ ow.} \\
            \textit{Out}_i = 1 \text{ if $i$ lives in the outskirts of a city and $0$ ow.} \\
            \textit{City}_i = 1 \text{ if $i$ lives in a City and $0$ ow.}
        \end{cases}
    \end{equation*}

    Do note that we should only include two of the three encoded variables (why?). The one we omit becomes our \textit{level of reference}.
    
    \begin{equation*}
        Y_i = \beta_0 + \beta_1 \textit{Age}_i + \beta_2 \textit{Out}_i + \beta_3 \textit{City}_i + \varepsilon_i
    \end{equation*}
\end{frame}

\begin{frame}{Multi-level Categorical Features}
    Let $\beta_2 = 1,000$ and $\beta_3 = 5000$.

    How do you interpret them?

    \begin{equation*}
        Y_i = \beta_0 + \beta_1 \textit{Age}_i + \beta_2 \textit{Out}_i + \beta_3 \textit{City}_i + \varepsilon_i
    \end{equation*}
    
    \begin{itemize}
        \pause
        \item On average, the expected income of people from the Outskirts is \$$1,000$ greater than in Rural areas.
        \pause
        \item On average, the expected income of people from the City is \$$1,000$ greater than in Rural areas.
        \pause
        \item On average, the expected income of people from the City is \$$4,000$ greater than in the Outskirts.
    \end{itemize}

    \begin{center}
        \textit{It's as if each group had its own intercept!}
    \end{center}
    $$E(Y_i | Age_i, City_i = 1) = (\beta_0 + \beta_3) + \beta_1 Age_i$$
\end{frame}

% FEs
\begin{frame}{Fixed Effects}
    Suppose that we now have multiple observations per individual.
    \begin{equation*}
        Y_{it} = \beta_0 + \beta_1 \textit{GRE}_{it} + \beta_2 \textit{IQ}_i + \varepsilon_{it}
    \end{equation*}

    \begin{itemize}
        \item Note that \textit{IQ} does not vary over time.
        \item \textit{IQ} is likely to be unobservable (omitted variable bias).
    \end{itemize}

    Our ideal data would look like this:
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \textbf{ID} & \textbf{Time ($t$)} & \textbf{Income ($Y$)} & \textbf{GRE} & \textbf{IQ} \\
            \hline
            1 & 1 & 45,000 & 154 & 110 \\
            1 & 2 & 47,500 & 155 & 110 \\
            1 & 3 & 50,000 & 162 & 110 \\
            \hline
            2 & 1 & 55,000 & 162 & 120 \\
            2 & 2 & 57,500 & 163 & 120 \\
            2 & 3 & 60,000 & 169 & 120 \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

% FEs 2
\begin{frame}{Fixed Effects}
    Assuming we cannot observe people's \textit{IQ}, we can assign an intercept to each individual to overcome this bias.
    \begin{equation*}
        \begin{split}
            Y_{it} & = \beta_0 + \beta^{(i)} + \beta_1 GRE_{it} + U_{it} \\
                & = \alpha_i + \beta_1 \textit{GRE}_{it} + U_{it}
        \end{split}
    \end{equation*}

    Again, it's like fitting a separate regression line to each individual while assuming the relationship between Income and \textit{IQ} is the same \textit{across} all individuals.
    \begin{itemize}
        \item $N$ regression lines, all with the same slope
        \item The \textit{fixed effect} of each individual absorbs the bias introduced by any unobserved or time-invariant factors specific to each individual
        \item We need to use \textit{clustered standard errors} because residuals \textit{within} each individual are likely to be correlated over time.
    \end{itemize}
\end{frame}

% FEs 3
\begin{frame}{Fixed Effects}
    As an informal way to see why this works, let's go back to simple linear regression.
    \begin{equation*}
        \min_{\{\beta_0, \beta_1\}} E\Big[(Y_i - \beta_0 - \beta_1 X_i )^2 \Big]
    \end{equation*}
    The first-order condition \textit{wrt} $\beta_0$ is
    \begin{equation*}
        E(Y_i) - E(\beta_0 + \beta_1 X_i) = E(\varepsilon_i) = 0
    \end{equation*}
    We pick $\beta_0$ such that the expected value of the errors is centered around zero.

    In a Fixed Effects model, $\alpha_i$ plays the same role in centering the residuals of each unit of observation $i$ around zero. Hence, it absorbs all unobserved or time-invariant factors.
\end{frame}

% FEs 4
\begin{frame}{STAR Experiment}
    \begin{table}[h!]
        \centering
        \footnotesize
        \begin{tabular}{lcccc}
            \hline
            \textbf{Explanatory Variable} & \textbf{(1)} & \textbf{(2)} & \textbf{(3)} & \textbf{(4)} \\
            \hline
            Small class                   & 4.82        & 5.37        & 5.36        & 5.37        \\
                                          & (2.19)      & (1.26)      & (1.21)      & (1.19)      \\
            Regular/aide class            & 0.12        & 0.29        & 0.53        & 0.31        \\
                                          & (2.23)      & (1.13)      & (1.09)      & (1.07)      \\
            White/Asian                   & —           & 8.35        & 8.44        & 8.44        \\
                                          &             & (1.35)      & (1.36)      & (1.36)      \\
            Girl                          & —           & 4.48        & 4.39        & 4.39        \\
                                          &             & (0.63)      & (0.63)      & (0.63)      \\
            Free lunch                    & —           & —           & -13.15      & -13.07      \\
                                          &             &             & (0.77)      & (0.77)      \\
            White teacher                 & —           & —           & —           & -0.57       \\
                                          &             &             &             & (2.10)      \\
            Teacher experience            & —           & —           & —           & 0.26        \\
                                          &             &             &             & (0.10)      \\
            Teacher Master's degree       & —           & —           & —           & -0.51       \\
                                          &             &             &             & (1.06)      \\
            \hline
            \textbf{School fixed effects} & No          & Yes         & Yes         & Yes         \\
            \textbf{R$^2$}                & 0.01        & 0.25        & 0.31        & 0.31        \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

% Extra
\begin{frame}{Interpreting Logistic Regression}
    \begin{equation*}
    \begin{split}
        P(Y_i = 1 | X_i) & = \frac{1}{1 + \exp(-X_i^T \beta)} \\
        \implies \frac{P(Y_i = 1 | X_i)}{1 - P(Y_i = 1 | X_i)} & = 
            \frac{
                \frac{1}{1 + \exp(-X_i^T \beta)}
            }{
                1 - \frac{1}{1 + \exp(-X_i^T \beta)}
            } \\
            & = \frac{
                \frac{1}{1 + \exp(-X_i^T \beta)}
            }{
                \frac{\exp(-X_i^T \beta)}{1 + \exp(-X_i^T \beta)}
            } \\
            & = \exp(X_i^T \beta) \\
        \implies \log \Big( \frac{P(Y_i = 1 | X_i)}{1 - P(Y_i = 1 | X_i)} \Big) & = X_i^T \beta
    \end{split}
    \end{equation*}
\end{frame}

\end{document}
